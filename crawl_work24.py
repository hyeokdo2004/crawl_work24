import requests, json, time
from bs4 import BeautifulSoup
from datetime import datetime

BASE_URL = "https://www.work24.go.kr"
HEADERS = {"User-Agent": "Mozilla/5.0"}
TIMEOUT = 20
STATE_FILE = "work24_state.json"

# ======================
# ëª¨ë“  ê²Œì‹œíŒ (ë„¤ê°€ ë§í•œ ê²ƒ ì „ë¶€)
# ======================
BOARD_CONFIGS = [
    {"name":"ê³ ìš©24 ê³µì§€ì‚¬í•­","list":"/cm/c/a/0100/selectBbttList.do","param":"ntceStno","extra":{"bbsClCd":"kf9cT1sUygs8E64dnqWAxg=="}},
    {"name":"ê³ ìš©24 ì´ë²¤íŠ¸","list":"/cm/c/e/0100/selectEvtList.do","param":"evtSeq","extra":{}},
    {"name":"ê³µì§€ì‚¬í•­ B1100","list":"/cm/c/b/1100/selectBbttList.do","param":"polySvcFomtId","extra":{}},
    {"name":"ê³µì§€ì‚¬í•­ 0130","list":"/cm/c/b/0130/selectBbttList.do","param":"ntceStno","extra":{"bbsClCd":"+WhIYyX4MTPwl6gr4E19tQ=="}},
    {"name":"ë‰´ìŠ¤ë ˆí„°","list":"/cm/c/d/0220/selectGatherNewsLetter.do","param":"ntceStno","extra":{"bbsClCd":"DnDyhlwrq2vGTpGw9B1HxQ=="}},
    {"name":"ì§ì—…í›ˆë ¨ ê³µì§€","list":"/cm/c/a/0410/selectBbttList.do","param":"ntceStno","extra":{"bbsClCd":"OosccI71O3P2dBxVz5A40Q=="}},

    {"name":"ìƒì„¸ì±„ìš©","list":"/wk/a/b/1200/retriveDtlEmpSrchList.do","param":"empSeq","extra":{}},
    {"name":"ë‚´ì£¼ë³€ì±„ìš©","list":"/wk/a/b/1600/retriveAroundMeEmpInfoList.do","param":"empSeq","extra":{}},
    {"name":"ì‚¬ì—…ê²€ìƒ‰","list":"/wk/a/d/1000/retrieveBusiSearch.do","param":"busiSeq","extra":{}},
    {"name":"ì±„ìš©í–‰ì‚¬","list":"/wk/a/f/1100/retrieveEmpEventList.do","param":"evtSeq","extra":{}},
    {"name":"ì˜¨ë¼ì¸ì±„ìš©ë°•ëŒíšŒ","list":"/wk/a/f/1100/retrieveOnlineEmpExhbList.do","param":"exhbSeq","extra":{}},
    {"name":"ê³ ìš©ë™í–¥","list":"/wk/r/e/1140/pictureEmpTrend.do","param":"ntceStno","extra":{}},
    {"name":"ê³ ìš©ë‰´ìŠ¤","list":"/wk/r/g/1110/retrieveEmpNewsList.do","param":"ntceStno","extra":{}},
]

# ======================
# ê³µí†µ í•¨ìˆ˜
# ======================
def load_state():
    try:
        with open(STATE_FILE,"r",encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

def save_state(state):
    with open(STATE_FILE,"w",encoding="utf-8") as f:
        json.dump(state,f,ensure_ascii=False,indent=2)

def extract_id(href):
    try:
        return href.split("'")[1]   # fn_DetailInfo('433','ERF') â†’ 433
    except:
        return None

def last_page(html):
    soup = BeautifulSoup(html,"html.parser")
    pages=[]
    for b in soup.select("button[onclick^='fn_Search']"):
        try:
            pages.append(int(b["onclick"].split("(")[1].split(")")[0]))
        except:
            pass
    return max(pages) if pages else 1

def extract_attachments(soup):
    files=[]
    for a in soup.select("a[onclick^='gfn_downloadAttFile3nd']"):
        try:
            args=a["onclick"].split("(")[1].split(")")[0].replace("'","").split(",")
            enc, fsno = args[0], args[1]
            url=f"{BASE_URL}/cm/common/fileDownload3nd.do?encAthflSeq={enc}&atchFsno={fsno}"
            files.append({"name":a.get_text(strip=True),"url":url})
        except:
            pass
    return files

# ======================
# ë©”ì¸
# ======================
state = load_state()
updated=False

for board in BOARD_CONFIGS:
    print(f"\nğŸ“Œ {board['name']} ì¦ë¶„ ìˆ˜ì§‘")
    state.setdefault(board["name"],{})

    params={"currentPageNo":1,"recordCountPerPage":10}
    params.update(board["extra"])

    r=requests.get(BASE_URL+board["list"],params=params,headers=HEADERS,timeout=TIMEOUT)
    lp=last_page(r.text)
    print(f"  ë§ˆì§€ë§‰ í˜ì´ì§€: {lp}")

    for p in range(1,lp+1):
        params["currentPageNo"]=p
        try:
            r=requests.get(BASE_URL+board["list"],params=params,headers=HEADERS,timeout=TIMEOUT)
            soup=BeautifulSoup(r.text,"html.parser")

            for a in soup.select("a[href^='javascript:fn_DetailInfo']"):
                pid=extract_id(a.get("href",""))
                if not pid or pid in state[board["name"]]:
                    continue

                detail_url=f"{BASE_URL}{board['list'].replace('List','Info')}?{board['param']}={pid}"
                dr=requests.get(detail_url,headers=HEADERS,timeout=TIMEOUT)
                dsoup=BeautifulSoup(dr.text,"html.parser")

                state[board["name"]][pid]={
                    "title":a.get_text(strip=True),
                    "detected_at":datetime.utcnow().isoformat(),
                    "detail_url":detail_url,
                    "attachments":extract_attachments(dsoup)
                }
                updated=True
                print(f"ğŸ“„ ì‹ ê·œ ê²Œì‹œë¬¼ {pid} / ì²¨ë¶€ {len(state[board['name']][pid]['attachments'])}")

        except Exception as e:
            print("âš ",e)
        time.sleep(1)

if updated:
    save_state(state)

print("\nâœ… GitHub ì¦ë¶„ ìˆ˜ì§‘ ì™„ë£Œ")
